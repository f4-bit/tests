#!/usr/bin/env bash

set -e  # Detener en caso de error

echo "ðŸš€ Configurando entorno Multi-GPU Llama.cpp API"
echo "================================================"

# 1. Verificar dependencias del sistema
echo "ðŸ” Verificando dependencias del sistema..."

# Verificar cmake
#!/bin/bash

# Verificar cmake
if ! command -v cmake &> /dev/null; then
    echo "âŒ CMake no encontrado. Instalando sin sudo..."
    
    # Crear directorio local para binarios si no existe
    mkdir -p ~/bin
    mkdir -p ~/local
    
    # MÃ©todo 1: Descargar binario precompilado de CMake
    echo "ðŸ“¥ Descargando CMake precompilado..."
    cd ~/local
    
    # Obtener la Ãºltima versiÃ³n estable (ajustar segÃºn necesidad)
    CMAKE_VERSION="3.27.7"
    CMAKE_ARCHIVE="cmake-${CMAKE_VERSION}-linux-x86_64"
    
    # Descargar si no existe
    if [ ! -f "${CMAKE_ARCHIVE}.tar.gz" ]; then
        wget "https://github.com/Kitware/CMake/releases/download/v${CMAKE_VERSION}/${CMAKE_ARCHIVE}.tar.gz"
    fi
    
    # Extraer
    if [ ! -d "$CMAKE_ARCHIVE" ]; then
        tar -xzf "${CMAKE_ARCHIVE}.tar.gz"
    fi
    
    # Crear enlace simbÃ³lico en ~/bin
    ln -sf ~/local/${CMAKE_ARCHIVE}/bin/cmake ~/bin/cmake
    ln -sf ~/local/${CMAKE_ARCHIVE}/bin/ctest ~/bin/ctest
    ln -sf ~/local/${CMAKE_ARCHIVE}/bin/cpack ~/bin/cpack
    
    # Agregar ~/bin al PATH si no estÃ¡
    if [[ ":$PATH:" != *":$HOME/bin:"* ]]; then
        echo 'export PATH="$HOME/bin:$PATH"' >> ~/.bashrc
        export PATH="$HOME/bin:$PATH"
    fi
    
    echo "âœ… CMake instalado en ~/bin/"
    echo "ðŸ”„ Ejecuta 'source ~/.bashrc' o reinicia la terminal"
    
else
    echo "âœ… CMake encontrado: $(cmake --version | head -n1)"
fi

# Verificar instalaciÃ³n
if command -v cmake &> /dev/null; then
    echo "âœ… CMake estÃ¡ disponible: $(which cmake)"
    echo "ðŸ“‹ VersiÃ³n: $(cmake --version | head -n1)"
else
    echo "âš ï¸  CMake instalado pero no en PATH. Ejecuta: source ~/.bashrc"
fi

# Verificar compilador C++
if ! command -v g++ &> /dev/null && ! command -v clang++ &> /dev/null; then
    echo "âŒ Compilador C++ no encontrado"
    exit 1
else
    echo "âœ… Compilador C++ disponible"
fi

# Verificar CUDA (opcional)
if command -v nvcc &> /dev/null; then
    CUDA_VERSION=$(nvcc --version | grep "release" | sed -n 's/.*release \([0-9.]*\).*/\1/p')
    echo "âœ… CUDA encontrado: v$CUDA_VERSION"
    USE_CUDA=true
else
    echo "âš ï¸  CUDA no encontrado. Compilando solo para CPU."
    USE_CUDA=false
fi

# Verificar nvidia-smi
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… nvidia-smi disponible"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader | while read line; do
        echo "   ðŸ“Œ GPU detectada: $line"
    done
else
    echo "âš ï¸  nvidia-smi no disponible"
fi

echo ""

# 2. Actualizar pip
echo "â¬†ï¸  Actualizando pip..."
pip install --upgrade pip

echo ""

# 3. Instalar dependencias Python
echo "ðŸ“¥ Instalando dependencias Python..."
pip install fastapi uvicorn aiohttp aiofiles sentence-transformers torch psutil httpx huggingface-hub

# Instalar PyTorch con CUDA si estÃ¡ disponible
if [ "$USE_CUDA" = true ]; then
    echo "ðŸ”¥ Instalando PyTorch con soporte CUDA..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
else
    echo "ðŸ’¾ Instalando PyTorch solo CPU..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
fi

echo ""

# 4. Clonar y compilar llama.cpp
echo "ðŸ“¦ Descargando y compilando llama.cpp..."

# Limpiar instalaciÃ³n previa si existe
if [ -d "llama.cpp" ]; then
    echo "ðŸ§¹ Limpiando instalaciÃ³n anterior..."
    rm -rf llama.cpp
fi

# Clonar repositorio
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# Crear directorio build
mkdir build
cd build

# Configurar compilaciÃ³n
echo "âš™ï¸  Configurando compilaciÃ³n..."
if [ "$USE_CUDA" = true ]; then
    echo "   ðŸ”¥ Compilando con soporte CUDA..."
    cmake .. \
        -DGGML_CUDA=ON \
        -DCMAKE_BUILD_TYPE=Release \
        -DGGML_CUDA_F16=ON
else
    echo "   ðŸ’¾ Compilando solo para CPU..."
    cmake .. \
        -DCMAKE_BUILD_TYPE=Release \
        -DGGML_NATIVE=ON
fi

# Compilar
echo "ðŸ”¨ Compilando llama.cpp (esto puede tardar varios minutos)..."
cmake --build . --config Release -j$(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 4)

# Verificar compilaciÃ³n
if [ -f "bin/llama-server" ]; then
    echo "âœ… llama-server compilado correctamente"
    
    # Crear symlink para compatibilidad
    cd ..  # Salir de build/
    ln -sf build/bin/llama-server server
    echo "ðŸ”— Creado symlink: llama.cpp/server -> build/bin/llama-server"
    
    # Probar que funciona
    echo "ðŸ§ª Probando servidor..."
    if ./server --help > /dev/null 2>&1; then
        echo "âœ… Servidor funciona correctamente"
    else
        echo "âš ï¸  Advertencia: Error probando el servidor"
    fi
else
    echo "âŒ Error: No se pudo compilar llama-server"
    exit 1
fi

cd ..  # Regresar al directorio raÃ­z

echo ""

# 5. Crear directorio para modelos
echo "ðŸ“ Creando directorio para modelos..."
mkdir -p models
echo "âœ… Directorio 'models' creado"

echo ""

# 6. InformaciÃ³n de instalaciÃ³n completada
echo "ðŸŽ‰ Â¡InstalaciÃ³n completada exitosamente!"
echo "========================================"
echo ""
echo "ðŸ“‹ Resumen de la instalaciÃ³n:"
echo "   âœ… CMake: $(cmake --version | head -n1)"
echo "   âœ… Python deps: FastAPI, PyTorch, sentence-transformers, etc."

if [ "$USE_CUDA" = true ]; then
    echo "   âœ… llama.cpp: Compilado con soporte CUDA"
else
    echo "   âœ… llama.cpp: Compilado para CPU"
fi

echo "   âœ… Servidor: ./llama.cpp/server (symlink a build/bin/llama-server)"
echo "   âœ… Directorio modelos: ./models/"
echo ""

# 7. Instrucciones de uso
echo "ðŸš€ Para ejecutar la API:"
echo "========================"
echo ""
echo "# Uso bÃ¡sico (detecta GPUs automÃ¡ticamente):"
echo "python main.py"
echo ""
echo "# Con configuraciÃ³n personalizada:"
echo "python main.py \\"
echo "    --server-binary ./llama.cpp/server \\"
echo "    --repo-id \"unsloth/Qwen2.5-Coder-32B-Instruct-GGUF\" \\"
echo "    --model-filename \"Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf\" \\"
echo "    --context-size 32768 \\"
echo "    --max-output-tokens 4096 \\"
echo "    --parallel-requests 3 \\"
echo "    --min-vram-gb 18 \\"
echo "    --max-gpus 1"
echo ""

# 8. InformaciÃ³n adicional
echo "ðŸ“š InformaciÃ³n adicional:"
echo "========================="
echo "â€¢ API principal: http://localhost:3000"
echo "â€¢ Health check: http://localhost:3001/health"
echo "â€¢ MÃ©tricas: http://localhost:3001/metrics"
echo "â€¢ Logs: Se mostrarÃ¡n en la consola"
echo "â€¢ Modelos se descargan en: ./models/"
echo ""

if [ "$USE_CUDA" = true ]; then
    echo "ðŸŽ® GPUs detectadas:"
    if command -v nvidia-smi &> /dev/null; then
        nvidia-smi --query-gpu=index,name,memory.total --format=csv,noheader | nl -v0 -s': GPU ' | sed 's/^[ \t]*/   /'
    fi
    echo ""
fi

echo "ðŸ’¡ Consejos:"
echo "============"
echo "â€¢ Primera ejecuciÃ³n descargarÃ¡ el modelo (~15-20GB)"
echo "â€¢ AsegÃºrate de tener suficiente espacio en disco"
echo "â€¢ Para mejores resultados usa SSD"
echo "â€¢ Monitorea el uso de VRAM con nvidia-smi"
echo ""
echo "âœ… Â¡Todo listo! Ejecuta 'python main.py' para comenzar."